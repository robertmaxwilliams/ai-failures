@article{yam2018historic,
	title = {Predicting future {AI} failures from historic examples},
	volume = {21},
	issn = {1463-6689},
	url = {https://doi.org/10.1108/FS-04-2018-0034},
	doi = {10.1108/FS-04-2018-0034},
	abstract = {Purpose The purpose of this paper is to explain to readers how intelligent systems can fail and how artificial intelligence (AI) safety is different from cybersecurity. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100 per cent secure system. Design/methodology/approach AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AI Safety, failures are at the same, moderate level of criticality as in cybersecurity; however, for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. Findings In this paper, the authors present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. The authors suggest that both the frequency and the seriousness of future AI failures will steadily increase. Originality/value This is a first attempt to assemble a public data set of AI failures and is extremely valuable to AI Safety researchers.},
	number = {1},
	urldate = {2021-01-29},
	journal = {foresight},
	author = {Yampolskiy, Roman V.},
	month = jan,
	year = {2019},
	keywords = {Cybersecurity, Failures},
	pages = {138--152},
}

@article{scott2020classification,
    author = {Peter J. Scott and Roman V. Yampolskiy},
    title = {Classification Schemas for Artificial Intelligence Failures},
    journal = {Delphi - Interdisciplinary Review of Emerging Technologies},
    volume = {2},
    number = {4},
    year = {2020},
    abstract = {In this paper we examine historical failures of artificial intelligence (AI) and propose a classification scheme for categorising future failures. By doing so we hope that (a) the responses to future failures can be improved through applying a systematic classification that can be used to simplify the choice of response and (b) future failures can be reduced through augmenting development lifecycles with targeted risk assessments.},
    url = {https://doi.org/10.21552/delphi/2019/4/8},
    doi = {10.21552/delphi/2019/4/8}
}

@article{uesato2018adversarial,
  author    = {Jonathan Uesato and
               Ananya Kumar and
               Csaba Szepesv{\'{a}}ri and
               Tom Erez and
               Avraham Ruderman and
               Keith Anderson and
               Krishnamurthy Dvijotham and
               Nicolas Heess and
               Pushmeet Kohli},
  title     = {Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures},
  journal   = {CoRR},
  volume    = {abs/1812.01647},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.01647},
  archivePrefix = {arXiv},
  eprint    = {1812.01647},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-01647.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% No idea how to understand or use this paper. I'm going to skip it.
@article{wallace2018landscapes,
     author = {Rodrick Wallace},
     title = {Failure of Real-Time Multi-Component, Multi-Level Cognitive Systems on Clausewitz
         Landscapes},
     journal = {The New York State Psychiatric Institute}, 
     year = {2018}
}

@article{anderson2005control,
    author = "Anderson, Brian D. O.",
    fjournal = "Communications in Information \& Systems",
    journal = "Commun. Inf. Syst.",
    number = "1",
    pages = "1--20",
    publisher = "International Press of Boston",
    title = "Failures of Adaptive Control Theory and their Resolution",
    url = "https://projecteuclid.org:443/euclid.cis/1149698471",
    volume = "5",
    year = "2005"
}

@article{cook1998complex,
    author= {Richard I. Cook},
    title= {How Complex Systems Fail},
    journal= {Cognitive Technologies Labratory},
    organization= {University of Chicago},
    year= {1998}
}

@article{carvin2017normal,
    author = {Stephanie Carvin},
    title = {Normal Autonomous Accidents: What happens when killer robots fail?},
    journal = {Carleton University},
    year = {2017},
    url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3161446}
}

@inproceedings{maas2018regulating,
    author = {Maas, Matthijs},
    year = {2018},
    month = {12},
    pages = {223-228},
    title = {Regulating for 'Normal AI Accidents': Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment},
    doi = {10.1145/3278721.3278766}
}

@misc{humbatova2019taxonomy,
    title={Taxonomy of Real Faults in Deep Learning Systems},
    author={Nargiz Humbatova and Gunel Jahangirova and Gabriele Bavota and Vincenzo Riccio and Andrea Stocco and Paolo Tonella},
    year={2019},
    eprint={1910.11015},
    archivePrefix={arXiv},
    primaryClass={cs.SE}
}

@book{virilio2007accident,
    title={The Original Accident},
    author={Paul Virilio},
    year={2007}
}


@article{shrivastava2009normal,
    author = {Samir Shrivastava and Karan Sonpar and Federica Pazzaglia},
    title ={Normal Accident Theory versus High Reliability Theory: 
        A resolution and call for an open systems view of accidents},
    journal = {Human Relations},
    volume = {62},
    number = {9},
    pages = {1357-1390},
    year = {2009},
    doi = {10.1177/0018726709339117},
    URL = {https://doi.org/10.1177/0018726709339117},
    eprint = {https://doi.org/10.1177/0018726709339117} ,
    abstract = { We resolve the longstanding debate between Normal Accident Theory (NAT) and High-Reliability Theory (HRT) by introducing a temporal dimension. Specifically, we explain that the two theories appear to diverge because they look at the accident phenomenon at different points of time. We, however, note that the debate’s resolution does not address the non-falsifiability problem that both NAT and HRT suffer from. Applying insights from the open systems perspective, we reframe NAT in a manner that helps the theory to address its non-falsifiability problem and factor in the role of humans in accidents. Finally, arguing that open systems theory can account for the conclusions reached by NAT and HRT, we proceed to offer pointers for future research to theoretically and empirically develop an open systems view of accidents. }
}

@inbook{perrow1999living,
 ISBN = {9780691004129},
 URL = {http://www.jstor.org/stable/j.ctt7srgf.7},
 abstract = {To make a systematic examination of the world of high-risk systems, and to address problems of reorganization of systems, risk analysis, and public participation, we need to carefully define our terms. Not everything untoward that happens should be called an accident; to exclude many minor failures, we need an exact definition of an accident. Our key term, system accident or normal accident, needs to be defined as precisely as possible, and distinguished from more commonplace accidents. We will define it with the aid of two concepts used loosely so far, which now require definition and illustration: complexity and coupling. We},
 author = {Charles Perrow},
 booktitle = {Normal Accidents: Living with High Risk Technologies - Updated Edition},
 edition = {REV - Revised},
 pages = {62--100},
 publisher = {Princeton University Press},
 title = {Complexity, Coupling, and Catastrophe},
 year = {1999}
}

@book{perrow1984living,
 ISBN = {046505143X},
 author = {Charles Perrow},
 booktitle = {Normal Accidents: Living with High Risk Technologies},
 title = {Normal Accidents: Living with High Risk Technologies},
 publisher = {Basic Books},
 year = {1984}
}

@article{lehman2018surprising,
author = {Lehman, Joel and others},
title = {The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities},
journal = {Artificial Life},
volume = {26},
number = {2},
pages = {274-306},
year = {2020},
doi = {10.1162/artl\_a\_00319},
    note ={PMID: 32271631},
URL = { https://doi.org/10.1162/artl_a_00319 },
eprint = { https://doi.org/10.1162/artl_a_00319 }
}
%author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Cheney, Nick and Chrabaszcz, Patryk and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fŕenoy, Antoine and Gagńe, Christian and Le Goff, Leni and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Schulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Watson, Richard and Weimer, Westley and Yosinski, Jason},



@article{bainbridge1983ironies,
        title = {Ironies of automation},
        journal = {Automatica},
        volume = {19},
        number = {6},
        pages = {775-779},
        year = {1983},
        issn = {0005-1098},
        doi = {https://doi.org/10.1016/0005-1098(83)90046-8},
        url = {https://www.sciencedirect.com/science/article/pii/0005109883900468},
        author = {Lisanne Bainbridge},
        keywords = {Control engineering computer applications, man-machine systems, on-line operation, process control, system failure and recovery},
        abstract = {This paper discusses the ways in which automation of industrial processes may expand rather than eliminate problems with the human operator. Some comments will be made on methods of alleviating these problems within the ‘classic’ approach of leaving the operator with responsibility for abnormal conditions, and on the potential for continued use of the human operator for on-line decision-making within human-computer collaboration.}
    }


@Article{oleary2008hype,
  author={O'Leary, Daniel E.},
  title={{Gartner's hype cycle and information system research issues}},
  journal={International Journal of Accounting Information Systems},
  year=2008,
  volume={9},
  number={4},
  pages={240-252},
  month={},
  keywords={Research opportunities; Information systems; Accounting information systems; Research issues; Hype c},
  doi={10.9016/j.accinf.2008.09.},
  abstract={This paper uses Gartner Group's hype cycle as a basis to analyze research issues and opportunities in information systems. The findings including, “where” we find a technology in the hype cycle can influence the kinds of research questions we can ask, the information available about that technology and the research methods that can be employed on the technology at that stage.},
  url={https://ideas.repec.org/a/eee/ijoais/v9y2008i4p240-252.html}
}

@Article{jayden2018sustainability,
author = {Khakurel, Jayden and Penzenstadler, Birgit and Porras, Jari and Knutas, Antti and Zhang, Wenlu},
title = {The Rise of Artificial Intelligence under the Lens of Sustainability},
journal = {Technologies},
volume = {6},
year = {2018},
number = {4},
article-number = {100},
url = {https://www.mdpi.com/2227-7080/6/4/100},
issn = {2227-7080},
abstract = {Since the 1950s, artificial intelligence (AI) has been a recurring topic in research. However, this field has only recently gained significant momentum because of the advances in technology and algorithms, along with new AI techniques such as machine learning methods for structured data, modern deep learning, and natural language processing for unstructured data. Although companies are eager to join the fray of this new AI trend and take advantage of its potential benefits, it is unclear what implications AI will have on society now and in the long term. Using the five dimensions of sustainability to structure the analysis, we explore the impacts of AI on several domains. We find that there is a significant impact on all five dimensions, with positive and negative impacts, and that value, collaboration, sharing responsibilities; ethics will play a vital role in any future sustainable development of AI in society. Our exploration provides a foundation for in-depth discussions and future research collaborations.},
doi = {10.3390/technologies6040100}
}

@inproceedings{weick1999reliability,
  title={Organizing for high reliability: Processes of collective mindfulness.},
  booktitle={Organizing for high reliability: Processes of collective mindfulness.},
  author={K. Weick and K. Sutcliffe and David Obstfeld},
  year={1999}
}

@article{boulding1956general,
    author = {Boulding, Kenneth E.},
    title = {General Systems Theory—The Skeleton of Science},
    journal = {Management Science},
    volume = {2},
    number = {3},
    pages = {197-208},
    year = {1956},
    doi = {10.1287/mnsc.2.3.197},
    URL = { https://doi.org/10.1287/mnsc.2.3.197 },
    eprint = { https://doi.org/10.1287/mnsc.2.3.197 } ,
    abstract = { In recent years increasing need has been felt for a body of systematic theoretical constructs which will discuss the general relationships of the empirical world. This is the quest of General Systems Theory. It does not seek, of course, to establish a single, self-contained “general theory of practically everything” which will replace all the special theories of particular disciplines. Such a theory would be almost without content, for we always pay for generality by sacrificing content, and all we can say about practically everything is almost nothing. Somewhere however between the specific that has no meaning and the general that has no content there must be, for each purpose and at each level of abstraction, an optimum degree of generality. It is the contention of the General Systems Theorists that this optimum degree of generality in theory is not always reached by the particular sciences. }
}

@article{andrei2017flash,
author = {Kirilenko, Andrei and Kyle, Albert S. and SAMADI, Mehrdad and Tuzun, Tugkan},
title = {The Flash Crash: High-Frequency Trading in an Electronic Market},
journal = {The Journal of Finance},
volume = {72},
number = {3},
pages = {967-998},
doi = {https://doi.org/10.1111/jofi.12498},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12498},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jofi.12498},
abstract = {ABSTRACT We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event—the Flash Crash—where a large automated selling program was rapidly executed in the E-mini S\&P 500 stock index futures market. Using audit trail transaction-level data for the E-mini on May 6 and the previous three days, we find that the trading pattern of the most active nondesignated intraday intermediaries (classified as High-Frequency Traders) did not change when prices fell during the Flash Crash.},
year = {2017}
}

@inproceedings{chrabaszcz2018qbert,
  title     = {Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari},
  author    = {Patryk Chrab\c{a}szcz and Ilya Loshchilov and Frank Hutter},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {1419--1426},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/197},
  url       = {https://doi.org/10.24963/ijcai.2018/197}
}


@inproceedings{james2016containment,
author = {Babcock, James and Kramar, Janos and Yampolskiy, Roman},
year = {2016},
month = {04},
pages = {},
title = {The AGI Containment Problem},
booktitle = {The AGI Containment Problem},
isbn = {978-3-319-41648-9},
doi = {10.1007/978-3-319-41649-6_6}
}

@inproceedings{reuben2007virtual,
  title={A Survey on Virtual Machine Security},
  booktitle={A Survey on Virtual Machine Security},
  author={J. Reuben},
  year={2007}
}

@article{amodei2016concrete,
  title={Concrete Problems in AI Safety},
  author={Dario Amodei and Chris Olah and J. Steinhardt and Paul F. Christiano and John Schulman and Dan Man{\'e}},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.06565}
}

@misc{hadfieldmenell2020inverse,
      title={Inverse Reward Design},
      author={Dylan Hadfield-Menell and Smitha Milli and Pieter Abbeel and Stuart Russell and Anca Dragan},
      year={2020},
      eprint={1711.02827},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}



% AI and non-AI long term future outcomes for humanity
@article{bostrom2001extinction,
author = {Bostrom, Dr},
year = {2001},
month = {11},
pages = {},
title = {Existential Risks - Analyzing Human Extinction Scenarios and Related Hazards},
volume = {9},
journal = {Journal of Evolution and Technology}
}


% Studies verifaction, validity, security, and control in modern and future AI/computer systems
@article{russell2015priorities,
title={Research Priorities for Robust and Beneficial Artificial Intelligence}, 
volume={36}, 
url={https://ojs.aaai.org/index.php/aimagazine/article/view/2577}, 
DOI={10.1609/aimag.v36i4.2577}, 
abstractNote={Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.}, 
number={4}, 
journal={AI Magazine}, 
author={Russell, Stuart and Dewey, Daniel and Tegmark, Max}, 
year={2015}, 
month={Dec.}, 
pages={105-114} 
}

% lays the groundwork for what becomes intrumental goals
@inproceedings{omohundro2008drives,
author = {Omohundro, Stephen M.},
title = {The Basic AI Drives},
year = {2008},
isbn = {9781586038335},
publisher = {IOS Press},
address = {NLD},
abstract = {One might imagine that AI systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of “drives” that will appear in sufficiently advanced AI systems of any design. We call them drives because they are tendencies which will be present unless explicitly counteracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also discuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to prevent themselves from being harmed. Finally we examine drives toward the acquisition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in designing intelligent technology which will lead to a positive future for humanity.},
booktitle = {Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference},
pages = {483–492},
numpages = {10},
keywords = {Artificial Intelligence, Rational Economic Behavior, Cognitive Drives, Self-Improving Systems, Utility Engineering}
}

% complicated paper but focused on risk/reward of AI for all humans
@inproceedings{yudkowsky2006posneg,
  title={Artificial Intelligence as a Positive and Negative Factor in Global Risk},
  author={Eliezer Yudkowsky},
  year={2006}
}

% good source for practical and future alignment
@inproceedings{taylor2020alignment,
  title={Alignment for Advanced Machine Learning Systems},
  author={Jessica Taylor and Eliezer Yudkowsky and Patrick LaVictoire and Andrew Critch},
  year={2020}
}

@inbook{soares2017foundations,
author = {Soares, Nate and Fallenstein, Benya},
year = {2017},
month = {05},
pages = {103-125},
title = {Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda},
publisher = {Machine Intelligence Research Institute},
isbn = {978-3-662-54031-2},
doi = {10.1007/978-3-662-54033-6_5}
}

@misc{leike2017gridworlds,
title={AI Safety Gridworlds}, 
author={Jan Leike and Miljan Martic and Victoria Krakovna and Pedro A. Ortega and Tom Everitt and Andrew Lefrancq and Laurent Orseau and Shane Legg},
year={2017},
eprint={1711.09883},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@book{bostrum2014superintelligence,
author = {Bostrom, Nick},
title = {Superintelligence: Paths, Dangers, Strategies},
year = {2014},
isbn = {0199678111},
publisher = {Oxford University Press, Inc.},
address = {USA},
edition = {1st},
abstract = {Superintelligence asks the questions: What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? Nick Bostrom lays the foundation for understanding the future of humanity and intelligent life. The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. If machine brains surpassed human brains in general intelligence, then this new superintelligence could become extremely powerful - possibly beyond our control. As the fate of the gorillas now depends more on humans than on the species itself, so would the fate of humankind depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed Artificial Intelligence, to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? This profoundly ambitious and original book breaks down a vast track of difficult intellectual terrain. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.}
}

% This is a beautiful and eleagent paper on AI safety. Yudkowsky is a genius.
@inproceedings{yudkowsky2011dwim,
  title={Complex value systems in friendly AI},
  author={Yudkowsky, Eliezer},
  booktitle={International Conference on Artificial General Intelligence},
  pages={388--393},
  year={2011},
  organization={Springer}
}

@inproceedings{fu2018adversarial,
title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},
author={Justin Fu and Katie Luo and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkHywl-A-},
}

@misc{brown2020gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% The yam sent me this, it's a pretty good perspective on a general AI technology
% interacting with society. Even if the faults are corrected it's still a bad idea 
% to use it, but the faults are still had to correct.
@article{10.1145/3446877,
author = {Marks, Paul},
title = {Can the Biases in Facial Recognition Be Fixed; Also, Should They?},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3446877},
doi = {10.1145/3446877},
abstract = {Many facial recognition systems used by law enforcement are shot through with biases. Can anything be done to make them fair and trustworthy?},
journal = {Commun. ACM},
month = feb,
pages = {20–22},
numpages = {3}
}
