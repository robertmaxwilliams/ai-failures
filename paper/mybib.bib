@article{yam2018historic,
	title = {Predicting future {AI} failures from historic examples},
	volume = {21},
	issn = {1463-6689},
	url = {https://doi.org/10.1108/FS-04-2018-0034},
	doi = {10.1108/FS-04-2018-0034},
	abstract = {Purpose The purpose of this paper is to explain to readers how intelligent systems can fail and how artificial intelligence (AI) safety is different from cybersecurity. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100 per cent secure system. Design/methodology/approach AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AI Safety, failures are at the same, moderate level of criticality as in cybersecurity; however, for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. Findings In this paper, the authors present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. The authors suggest that both the frequency and the seriousness of future AI failures will steadily increase. Originality/value This is a first attempt to assemble a public data set of AI failures and is extremely valuable to AI Safety researchers.},
	number = {1},
	urldate = {2021-01-29},
	journal = {foresight},
	author = {Yampolskiy, Roman V.},
	month = jan,
	year = {2019},
	keywords = {Cybersecurity, Failures},
	pages = {138--152},
}

@article{scott2020classification,
    author = {Peter J. Scott and Roman V. Yampolskiy},
    title = {Classification Schemas for Artificial Intelligence Failures},
    journal = {Delphi - Interdisciplinary Review of Emerging Technologies},
    volume = {2},
    number = {4},
    year = {2020},
    abstract = {In this paper we examine historical failures of artificial intelligence (AI) and propose a classification scheme for categorising future failures. By doing so we hope that (a) the responses to future failures can be improved through applying a systematic classification that can be used to simplify the choice of response and (b) future failures can be reduced through augmenting development lifecycles with targeted risk assessments.},
    url = {https://doi.org/10.21552/delphi/2019/4/8},
    doi = {10.21552/delphi/2019/4/8}
}

@article{uesato2018adversarial,
  author    = {Jonathan Uesato and
               Ananya Kumar and
               Csaba Szepesv{\'{a}}ri and
               Tom Erez and
               Avraham Ruderman and
               Keith Anderson and
               Krishnamurthy Dvijotham and
               Nicolas Heess and
               Pushmeet Kohli},
  title     = {Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures},
  journal   = {CoRR},
  volume    = {abs/1812.01647},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.01647},
  archivePrefix = {arXiv},
  eprint    = {1812.01647},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-01647.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% No idea how to understand or use this paper. I'm going to skip it.
@article{wallace2018landscapes,
     author = {Rodrick Wallace},
     title = {Failure of Real-Time Multi-Component, Multi-Level Cognitive Systems on Clausewitz
         Landscapes},
     journal = {The New York State Psychiatric Institute}, 
     year = {2018}
}

@article{anderson2005control,
    author = "Anderson, Brian D. O.",
    fjournal = "Communications in Information & Systems",
    journal = "Commun. Inf. Syst.",
    number = "1",
    pages = "1--20",
    publisher = "International Press of Boston",
    title = "Failures of Adaptive Control Theory and their Resolution",
    url = "https://projecteuclid.org:443/euclid.cis/1149698471",
    volume = "5",
    year = "2005"
}

@article{cook1998complex,
    author= {Richard I. Cook},
    title= {How Complex Systems Fail},
    journal= {Cognitive Technologies Labratory},
    organization= {University of Chicago},
    year= {1998}
}

@article{carvin2017normal,
    author = {Stephanie Carvin},
    title = {Normal Autonomous Accidents: What happens when killer robots fail?},
    journal = {Carleton University},
    year = {2017},
    url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3161446}
}

@inproceedings{maas2018regulating,
    author = {Maas, Matthijs},
    year = {2018},
    month = {12},
    pages = {223-228},
    title = {Regulating for 'Normal AI Accidents': Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment},
    doi = {10.1145/3278721.3278766}
}

@misc{humbatova2019taxonomy,
    title={Taxonomy of Real Faults in Deep Learning Systems},
    author={Nargiz Humbatova and Gunel Jahangirova and Gabriele Bavota and Vincenzo Riccio and Andrea Stocco and Paolo Tonella},
    year={2019},
    eprint={1910.11015},
    archivePrefix={arXiv},
    primaryClass={cs.SE}
}

@book{virilio2007accident,
    title={The Original Accident},
    author={Paul Virilio},
    year={2007}
}


@article{shrivastave2009normal,
    author = {Samir Shrivastava and Karan Sonpar and Federica Pazzaglia},
    title ={Normal Accident Theory versus High Reliability Theory: 
        A resolution and call for an open systems view of accidents},
    journal = {Human Relations},
    volume = {62},
    number = {9},
    pages = {1357-1390},
    year = {2009},
    doi = {10.1177/0018726709339117},
    URL = {https://doi.org/10.1177/0018726709339117},
    eprint = {https://doi.org/10.1177/0018726709339117} ,
    abstract = { We resolve the longstanding debate between Normal Accident Theory (NAT) and High-Reliability Theory (HRT) by introducing a temporal dimension. Specifically, we explain that the two theories appear to diverge because they look at the accident phenomenon at different points of time. We, however, note that the debate’s resolution does not address the non-falsifiability problem that both NAT and HRT suffer from. Applying insights from the open systems perspective, we reframe NAT in a manner that helps the theory to address its non-falsifiability problem and factor in the role of humans in accidents. Finally, arguing that open systems theory can account for the conclusions reached by NAT and HRT, we proceed to offer pointers for future research to theoretically and empirically develop an open systems view of accidents. }
}

@inbook{perrow1999living,
 ISBN = {9780691004129},
 URL = {http://www.jstor.org/stable/j.ctt7srgf.7},
 abstract = {To make a systematic examination of the world of high-risk systems, and to address problems of reorganization of systems, risk analysis, and public participation, we need to carefully define our terms. Not everything untoward that happens should be called an accident; to exclude many minor failures, we need an exact definition of an accident. Our key term, system accident or normal accident, needs to be defined as precisely as possible, and distinguished from more commonplace accidents. We will define it with the aid of two concepts used loosely so far, which now require definition and illustration: complexity and coupling. We},
 author = {Charles Perrow},
 booktitle = {Normal Accidents: Living with High Risk Technologies - Updated Edition},
 edition = {REV - Revised},
 pages = {62--100},
 publisher = {Princeton University Press},
 title = {Complexity, Coupling, and Catastrophe},
 year = {1999}
}

@book{perrow1984living,
 ISBN = {046505143X},
 author = {Charles Perrow},
 booktitle = {Normal Accidents: Living with High Risk Technologies},
 publisher = {Basic Books},
 year = {1984}
}








@article{lehman2018surprising,
author = {Lehman, Joel and others},
title = {The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities},
journal = {Artificial Life},
volume = {26},
number = {2},
pages = {274-306},
year = {2020},
doi = {10.1162/artl\_a\_00319},
    note ={PMID: 32271631},
URL = { https://doi.org/10.1162/artl_a_00319 },
eprint = { https://doi.org/10.1162/artl_a_00319 }
}
%author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Cheney, Nick and Chrabaszcz, Patryk and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fŕenoy, Antoine and Gagńe, Christian and Le Goff, Leni and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Schulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Watson, Richard and Weimer, Westley and Yosinski, Jason},



@article{bainbridge1983ironies,
        title = {Ironies of automation},
        journal = {Automatica},
        volume = {19},
        number = {6},
        pages = {775-779},
        year = {1983},
        issn = {0005-1098},
        doi = {https://doi.org/10.1016/0005-1098(83)90046-8},
        url = {https://www.sciencedirect.com/science/article/pii/0005109883900468},
        author = {Lisanne Bainbridge},
        keywords = {Control engineering computer applications, man-machine systems, on-line operation, process control, system failure and recovery},
        abstract = {This paper discusses the ways in which automation of industrial processes may expand rather than eliminate problems with the human operator. Some comments will be made on methods of alleviating these problems within the ‘classic’ approach of leaving the operator with responsibility for abnormal conditions, and on the potential for continued use of the human operator for on-line decision-making within human-computer collaboration.}
    }


@Article{oleary2008hype,
  author={O'Leary, Daniel E.},
  title={{Gartner's hype cycle and information system research issues}},
  journal={International Journal of Accounting Information Systems},
  year=2008,
  volume={9},
  number={4},
  pages={240-252},
  month={},
  keywords={Research opportunities; Information systems; Accounting information systems; Research issues; Hype c},
  doi={10.9016/j.accinf.2008.09.},
  abstract={This paper uses Gartner Group's hype cycle as a basis to analyze research issues and opportunities in information systems. The findings including, “where” we find a technology in the hype cycle can influence the kinds of research questions we can ask, the information available about that technology and the research methods that can be employed on the technology at that stage.},
  url={https://ideas.repec.org/a/eee/ijoais/v9y2008i4p240-252.html}
}


@Article{jayden2018sustainability,
author = {Khakurel, Jayden and Penzenstadler, Birgit and Porras, Jari and Knutas, Antti and Zhang, Wenlu},
title = {The Rise of Artificial Intelligence under the Lens of Sustainability},
journal = {Technologies},
volume = {6},
year = {2018},
number = {4},
article-number = {100},
url = {https://www.mdpi.com/2227-7080/6/4/100},
issn = {2227-7080},
abstract = {Since the 1950s, artificial intelligence (AI) has been a recurring topic in research. However, this field has only recently gained significant momentum because of the advances in technology and algorithms, along with new AI techniques such as machine learning methods for structured data, modern deep learning, and natural language processing for unstructured data. Although companies are eager to join the fray of this new AI trend and take advantage of its potential benefits, it is unclear what implications AI will have on society now and in the long term. Using the five dimensions of sustainability to structure the analysis, we explore the impacts of AI on several domains. We find that there is a significant impact on all five dimensions, with positive and negative impacts, and that value, collaboration, sharing responsibilities; ethics will play a vital role in any future sustainable development of AI in society. Our exploration provides a foundation for in-depth discussions and future research collaborations.},
doi = {10.3390/technologies6040100}
}




@inproceedings{weick1999reliability,
  title={Organizing for high reliability: Processes of collective mindfulness.},
  author={K. Weick and K. Sutcliffe and David Obstfeld},
  year={1999}
}







@article{boulding1956general,
    author = {Boulding, Kenneth E.},
    title = {General Systems Theory—The Skeleton of Science},
    journal = {Management Science},
    volume = {2},
    number = {3},
    pages = {197-208},
    year = {1956},
    doi = {10.1287/mnsc.2.3.197},
    URL = { https://doi.org/10.1287/mnsc.2.3.197 },
    eprint = { https://doi.org/10.1287/mnsc.2.3.197 } ,
    abstract = { In recent years increasing need has been felt for a body of systematic theoretical constructs which will discuss the general relationships of the empirical world. This is the quest of General Systems Theory. It does not seek, of course, to establish a single, self-contained “general theory of practically everything” which will replace all the special theories of particular disciplines. Such a theory would be almost without content, for we always pay for generality by sacrificing content, and all we can say about practically everything is almost nothing. Somewhere however between the specific that has no meaning and the general that has no content there must be, for each purpose and at each level of abstraction, an optimum degree of generality. It is the contention of the General Systems Theorists that this optimum degree of generality in theory is not always reached by the particular sciences. }
}



